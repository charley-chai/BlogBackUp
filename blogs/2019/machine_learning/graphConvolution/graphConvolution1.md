# 图卷积神经网络

## 历史脉络

- Franco于2005年提出
- GNN最早主要解决如分子结构分类等严格图论问题
- 2013年Bruna提出图上的基于频域和空域的卷积神经网络
- 图表示学习，word2vec，DeepWalk，TransE

## 图神经网络

基于**不动点**理论，给定一个图，用$x_v$表示节点$v$的特征，用$x(v,u)$表示边的特征。GNN的学习目标是获取每个节点的隐含状态$h_v$(state embedding)。GNN通过**迭代式更新**所有节点的隐含状态来使得每一个节点可以感知到其他点。

假设有节点$v$和节点$u, w$分别关联，那么节点$v$的状态更新公式姐可以表示为：

$$h_{v_{t+1}} = f(x_v, x(v, u), x(v, w), h_u, h_w, x_u, x_w)$$

更一般的，使用$x_{co[v]}$表示与$v$相关的边的特征队列，用$h_{nte[v]}$表示与$v$相邻的顶点的隐含特征，$x_{ne[v]}$表达与$v$相邻的点的特征，那么有：

$$h_{v_{t+1}} = f(x_v, x_{co[v]}, h_{nte[v]}, x_{ne[v]})$$

思路很简单，不断使用邻居信息对自己的状态进行更新，一直到状态趋于平稳。

## 不动点理论

**压缩映射**指：

$$d(F(x),F(y)) \leq cd(x,y), c\in[0,1)$$

也就是说，空间经过$F$变换之后，缩小了，从而经过不断迭代将所有点映射到一个点上面。

## 具体实现

$F$可以通过一个**前馈神经网络**即可实现，
